{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNtt0f5wukcRAdhZQbW5WGA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"15e0f8784bd04fceac4bccab70ff777f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3319b39fe8514ff389752a51b5c9678f","IPY_MODEL_d2a85054f62b4810b1c316d7ab4d7798","IPY_MODEL_71dd97b4462d424aae631975d4eb77e6"],"layout":"IPY_MODEL_d332bf81914a461ab1859fa0984a0cf5"}},"3319b39fe8514ff389752a51b5c9678f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a976192c3cfb4a49aaff706f46782b3c","placeholder":"​","style":"IPY_MODEL_ceac99aa6f0e4003b20e1c8259f46ef9","value":"Downloading: 100%"}},"d2a85054f62b4810b1c316d7ab4d7798":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a7160c7a45847c6817c196af9901468","max":443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aecb2c2fdfbd4a319ddca17fbaf2cb67","value":443}},"71dd97b4462d424aae631975d4eb77e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_889acb0d65e643cc86268dbe104b57bb","placeholder":"​","style":"IPY_MODEL_b43d6f3de9754de8a80d16defd951668","value":" 443/443 [00:00&lt;00:00, 9.77kB/s]"}},"d332bf81914a461ab1859fa0984a0cf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a976192c3cfb4a49aaff706f46782b3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ceac99aa6f0e4003b20e1c8259f46ef9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a7160c7a45847c6817c196af9901468":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aecb2c2fdfbd4a319ddca17fbaf2cb67":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"889acb0d65e643cc86268dbe104b57bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b43d6f3de9754de8a80d16defd951668":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45742ba0e52e4886ac3b95764973f8a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1b80531c49b430db8c0658b15dd267f","IPY_MODEL_ad909b0811114d97930fc8ab050e9509","IPY_MODEL_462541b64c0141ada7af2437b62816f0"],"layout":"IPY_MODEL_673a7680896144ef8e336119638f46ef"}},"a1b80531c49b430db8c0658b15dd267f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ef8f73cc9764360b609efb479106314","placeholder":"​","style":"IPY_MODEL_b75a4ee8813a4db58ea8fda69bfc4215","value":"Downloading: 100%"}},"ad909b0811114d97930fc8ab050e9509":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_abf8ccd3a42a4ed8befd731c9148c0bd","max":1340675298,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db5f946651834ddbac4d00a8224d1487","value":1340675298}},"462541b64c0141ada7af2437b62816f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d460edf91044d928c50be6e003aac7f","placeholder":"​","style":"IPY_MODEL_c7970c2432b74897b84349dc5fd9b62c","value":" 1.34G/1.34G [00:40&lt;00:00, 37.7MB/s]"}},"673a7680896144ef8e336119638f46ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ef8f73cc9764360b609efb479106314":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b75a4ee8813a4db58ea8fda69bfc4215":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abf8ccd3a42a4ed8befd731c9148c0bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db5f946651834ddbac4d00a8224d1487":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9d460edf91044d928c50be6e003aac7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7970c2432b74897b84349dc5fd9b62c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef75b29b3eb340d8aebd6c548f4d569a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bee8e809ae9c4843a85315cf4452e60e","IPY_MODEL_e7562096e57d45968c4f153ef0988230","IPY_MODEL_aab3ec884e134c349f2be76d04d4adb3"],"layout":"IPY_MODEL_09e48141e5b043a5854c9e08a7c472d1"}},"bee8e809ae9c4843a85315cf4452e60e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fdb1645e91b434cb584b60d60f5d0d8","placeholder":"​","style":"IPY_MODEL_fce3d21328fb4dd3b2e97247812b00ec","value":"Downloading: 100%"}},"e7562096e57d45968c4f153ef0988230":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_43f5f9965f2b498cbf0afd296a52b7be","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3d213310776b43a2900c97d91f9a1f93","value":231508}},"aab3ec884e134c349f2be76d04d4adb3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a284ca3dd5da4416b9bb8705205d8ba3","placeholder":"​","style":"IPY_MODEL_890ae00c1a604314864546eee6b5ff78","value":" 232k/232k [00:00&lt;00:00, 928kB/s]"}},"09e48141e5b043a5854c9e08a7c472d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fdb1645e91b434cb584b60d60f5d0d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fce3d21328fb4dd3b2e97247812b00ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43f5f9965f2b498cbf0afd296a52b7be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d213310776b43a2900c97d91f9a1f93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a284ca3dd5da4416b9bb8705205d8ba3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"890ae00c1a604314864546eee6b5ff78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# **Modele BERT** \n","**Réponse aux questions avec un BERT affiné**\n","\n","Représentations d'encodeurs bidirectionnels à partir de transformateurs\n","\n","Au lieu de regarder les mots isolément, BERT, un modèle basé sur un transformateur, tente d'utiliser le contexte des mots pour obtenir des incorporations. BERT utilise plusieurs concepts d'apprentissage en profondeur pour proposer un modèle qui examine le contexte de manière bidirectionnelle, en tirant parti des informations de l'ensemble des phrases dans leur ensemble grâce à l'attention personnelle."],"metadata":{"id":"YgCWfa36gasN"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quFBHONy-OG4","executionInfo":{"status":"ok","timestamp":1668074382382,"user_tz":-60,"elapsed":11371,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"aa93f384-45a0-4364-f12d-56d251f45649"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.1.0\n","  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n","\u001b[K     |████████████████████████████████| 884 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2022.6.2)\n","Collecting tokenizers==0.8.1.rc2\n","  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 39.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (4.64.1)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 47.8 MB/s \n","\u001b[?25hCollecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 56.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (21.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (1.21.6)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.2.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=0e9c2e5b243a728ec24a8518d52933d39c10121934e308a0ac1a1e06a1df800f\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.8.1rc2 transformers-3.1.0\n"]}],"source":["!pip install transformers==3.1.0"]},{"cell_type":"markdown","source":["Télechagement d'un modèle BERT pré-entrainé. Celui-ci est issue de la base de données SQUAD. \n","uncased= pas de différence entre miniscule et majuscule"],"metadata":{"id":"z4_6ixUGhMbC"}},{"cell_type":"code","source":["import torch\n","from transformers import BertForQuestionAnswering\n","model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["15e0f8784bd04fceac4bccab70ff777f","3319b39fe8514ff389752a51b5c9678f","d2a85054f62b4810b1c316d7ab4d7798","71dd97b4462d424aae631975d4eb77e6","d332bf81914a461ab1859fa0984a0cf5","a976192c3cfb4a49aaff706f46782b3c","ceac99aa6f0e4003b20e1c8259f46ef9","5a7160c7a45847c6817c196af9901468","aecb2c2fdfbd4a319ddca17fbaf2cb67","889acb0d65e643cc86268dbe104b57bb","b43d6f3de9754de8a80d16defd951668","45742ba0e52e4886ac3b95764973f8a7","a1b80531c49b430db8c0658b15dd267f","ad909b0811114d97930fc8ab050e9509","462541b64c0141ada7af2437b62816f0","673a7680896144ef8e336119638f46ef","9ef8f73cc9764360b609efb479106314","b75a4ee8813a4db58ea8fda69bfc4215","abf8ccd3a42a4ed8befd731c9148c0bd","db5f946651834ddbac4d00a8224d1487","9d460edf91044d928c50be6e003aac7f","c7970c2432b74897b84349dc5fd9b62c"]},"id":"o5MkcjhP-gEr","executionInfo":{"status":"ok","timestamp":1668074497473,"user_tz":-60,"elapsed":58457,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"9ee6b7c4-4671-4e76-c484-15d2817ade14"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/443 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15e0f8784bd04fceac4bccab70ff777f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45742ba0e52e4886ac3b95764973f8a7"}},"metadata":{}}]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","# Téléchargement d'un tokenizer préentrainé"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["ef75b29b3eb340d8aebd6c548f4d569a","bee8e809ae9c4843a85315cf4452e60e","e7562096e57d45968c4f153ef0988230","aab3ec884e134c349f2be76d04d4adb3","09e48141e5b043a5854c9e08a7c472d1","5fdb1645e91b434cb584b60d60f5d0d8","fce3d21328fb4dd3b2e97247812b00ec","43f5f9965f2b498cbf0afd296a52b7be","3d213310776b43a2900c97d91f9a1f93","a284ca3dd5da4416b9bb8705205d8ba3","890ae00c1a604314864546eee6b5ff78"]},"id":"AZx3nbNZ-gG6","executionInfo":{"status":"ok","timestamp":1668075060274,"user_tz":-60,"elapsed":836,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"791b35bd-9d73-41b5-e9f0-089e471a60b4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef75b29b3eb340d8aebd6c548f4d569a"}},"metadata":{}}]},{"cell_type":"code","source":["pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XTWMUhJF-7wz","executionInfo":{"status":"ok","timestamp":1668075066309,"user_tz":-60,"elapsed":4214,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"f21baeff-1498-4134-ac22-37fdfb5c37e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n"]}]},{"cell_type":"markdown","source":["Création d'une variable pdf_txt qui contient un texte. Nous pouvons aussi extraire le texte directement d'un pdf comme pour le modele word2vec avec les commandes suivantes: \n","\n","'import pdfplumber\n","\n","pdf = pdfplumber.open('lic_policy.pdf')\n","\n","page = pdf.pages[0]\n","\n","page1 = pdf.pages[1]\n","\n","pdf_txt = page.extract_text() + page1.extract_text()\n","\n","pdf.close()'"],"metadata":{"id":"iMH1v9_oiKN3"}},{"cell_type":"code","source":["pdf_txt= \"\"\" A chatbot is a computer program that simulates human conversation through voice commands or text chats or both. Chatbot, short for chatterbot, is an artificial intelligence (AI) feature that can be embedded and used through any major messaging application.\n","\n","There are a number of synonyms for chatbot, including \"talkbot,\" \"bot,\" \"IM bot,\" \"interactive agent\" or \"artificial conversation entity.\"  The progressive advance of technology has seen an increase in businesses moving from traditional to digital platforms to transact with consumers. Convenience through technology is being carried out by businesses by implementing AI techniques on their digital platforms. One AI technique that is growing in its application and use is chatbots. Some examples of chatbot technology are virtual assistants like Amazon's Alexa and Google Assistant, and messaging apps, such as WeChat and Facebook's Messenger.\n","\n","A chatbot is an automated program that interacts with customers as a human would and costs little to nothing to engage with. Chatbots attend to customers at all times of the day and week and are not limited by time or a physical location. This makes its implementation appealing to a lot of businesses that may not have the manpower or financial resources to keep employees working around the clock. \n","Chatbots are convenient for providing customer service and support 24 hours a day, 7 days a week. They also free up phone lines and are far less expensive over the long run than hiring people to perform support. Using AI and natural language processing, chatbots are becoming better at understanding what customers want and providing the help they need. Companies also like chatbots because they can collect data about customer queries, response times, satisfaction, and so on.\n","\n","Chatbots, however, are still limited. Even with natural language processing, they may not fully comprehend a customer's input and may provide incoherent answers. Many chatbots are also limited in the scope of queries that they are able to respond to. This may lead to frustration with a lack of emotion, sympathy, and personalization given fairly generic feedback. In addition to customer dissatisfaction with not reaching a human being, chatbots can be expensive to implement and maintain, especially if they must be customized and updated often.\n","\n"," \"\"\"\n","import nltk\n","nltk.download('punkt')\n","tokens = nltk.sent_tokenize(pdf_txt)\n","for t in tokens:\n","    print(t, \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lw-5_1n0OMT6","executionInfo":{"status":"ok","timestamp":1668078542479,"user_tz":-60,"elapsed":5,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"b12684cf-2455-4a3d-b057-6c2d56597dec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" A chatbot is a computer program that simulates human conversation through voice commands or text chats or both. \n","\n","Chatbot, short for chatterbot, is an artificial intelligence (AI) feature that can be embedded and used through any major messaging application. \n","\n","There are a number of synonyms for chatbot, including \"talkbot,\" \"bot,\" \"IM bot,\" \"interactive agent\" or \"artificial conversation entity.\" \n","\n","The progressive advance of technology has seen an increase in businesses moving from traditional to digital platforms to transact with consumers. \n","\n","Convenience through technology is being carried out by businesses by implementing AI techniques on their digital platforms. \n","\n","One AI technique that is growing in its application and use is chatbots. \n","\n","Some examples of chatbot technology are virtual assistants like Amazon's Alexa and Google Assistant, and messaging apps, such as WeChat and Facebook's Messenger. \n","\n","A chatbot is an automated program that interacts with customers as a human would and costs little to nothing to engage with. \n","\n","Chatbots attend to customers at all times of the day and week and are not limited by time or a physical location. \n","\n","This makes its implementation appealing to a lot of businesses that may not have the manpower or financial resources to keep employees working around the clock. \n","\n","Chatbots are convenient for providing customer service and support 24 hours a day, 7 days a week. \n","\n","They also free up phone lines and are far less expensive over the long run than hiring people to perform support. \n","\n","Using AI and natural language processing, chatbots are becoming better at understanding what customers want and providing the help they need. \n","\n","Companies also like chatbots because they can collect data about customer queries, response times, satisfaction, and so on. \n","\n","Chatbots, however, are still limited. \n","\n","Even with natural language processing, they may not fully comprehend a customer's input and may provide incoherent answers. \n","\n","Many chatbots are also limited in the scope of queries that they are able to respond to. \n","\n","This may lead to frustration with a lack of emotion, sympathy, and personalization given fairly generic feedback. \n","\n","In addition to customer dissatisfaction with not reaching a human being, chatbots can be expensive to implement and maintain, especially if they must be customized and updated often. \n","\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["question = \"What are advantages of chatbot ?\""],"metadata":{"id":"S_SbEyeB-gLS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenisation du texte"],"metadata":{"id":"qFWz20ULjcX4"}},{"cell_type":"code","source":["input_ids = tokenizer.encode(question, pdf_txt, max_length=512, truncation=True)\n","print('The input has a total of {:} tokens.'.format(len(input_ids)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zKzladEf-gNz","executionInfo":{"status":"ok","timestamp":1668078586971,"user_tz":-60,"elapsed":195,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"3fa8fe57-7299-401b-d105-ea39ee02c533"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The input has a total of 459 tokens.\n"]}]},{"cell_type":"code","source":["Impression des jetons avec et de leurs identifiants pour voir exactement ce que fait le tokenizer"],"metadata":{"id":"VhC3uQVvjpt1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","for token, id in zip(tokens, input_ids):\n","    if id == tokenizer.sep_token_id:\n","        print('')\n","    print('{:<12} {:>6,}'.format(token, id))\n","\n","    if id == tokenizer.sep_token_id:\n","        print('')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Vzj7DY5-gPq","executionInfo":{"status":"ok","timestamp":1668078589654,"user_tz":-60,"elapsed":603,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"63b5b86f-8919-4d84-b893-516b40ffbe92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]           101\n","what          2,054\n","are           2,024\n","advantages   12,637\n","of            1,997\n","chat         11,834\n","##bot        18,384\n","?             1,029\n","\n","[SEP]           102\n","\n","a             1,037\n","chat         11,834\n","##bot        18,384\n","is            2,003\n","a             1,037\n","computer      3,274\n","program       2,565\n","that          2,008\n","simulate     26,633\n","##s           2,015\n","human         2,529\n","conversation  4,512\n","through       2,083\n","voice         2,376\n","commands     10,954\n","or            2,030\n","text          3,793\n","chat         11,834\n","##s           2,015\n","or            2,030\n","both          2,119\n",".             1,012\n","chat         11,834\n","##bot        18,384\n",",             1,010\n","short         2,460\n","for           2,005\n","chatter      24,691\n","##bot        18,384\n",",             1,010\n","is            2,003\n","an            2,019\n","artificial    7,976\n","intelligence  4,454\n","(             1,006\n","ai            9,932\n",")             1,007\n","feature       3,444\n","that          2,008\n","can           2,064\n","be            2,022\n","embedded     11,157\n","and           1,998\n","used          2,109\n","through       2,083\n","any           2,151\n","major         2,350\n","messaging    24,732\n","application   4,646\n",".             1,012\n","there         2,045\n","are           2,024\n","a             1,037\n","number        2,193\n","of            1,997\n","synonym      10,675\n","##s           2,015\n","for           2,005\n","chat         11,834\n","##bot        18,384\n",",             1,010\n","including     2,164\n","\"             1,000\n","talk          2,831\n","##bot        18,384\n",",             1,010\n","\"             1,000\n","\"             1,000\n","bot          28,516\n",",             1,010\n","\"             1,000\n","\"             1,000\n","im           10,047\n","bot          28,516\n",",             1,010\n","\"             1,000\n","\"             1,000\n","interactive   9,123\n","agent         4,005\n","\"             1,000\n","or            2,030\n","\"             1,000\n","artificial    7,976\n","conversation  4,512\n","entity        9,178\n",".             1,012\n","\"             1,000\n","the           1,996\n","progressive   6,555\n","advance       5,083\n","of            1,997\n","technology    2,974\n","has           2,038\n","seen          2,464\n","an            2,019\n","increase      3,623\n","in            1,999\n","businesses    5,661\n","moving        3,048\n","from          2,013\n","traditional   3,151\n","to            2,000\n","digital       3,617\n","platforms     7,248\n","to            2,000\n","trans         9,099\n","##act        18,908\n","with          2,007\n","consumers    10,390\n",".             1,012\n","convenience  15,106\n","through       2,083\n","technology    2,974\n","is            2,003\n","being         2,108\n","carried       3,344\n","out           2,041\n","by            2,011\n","businesses    5,661\n","by            2,011\n","implementing 14,972\n","ai            9,932\n","techniques    5,461\n","on            2,006\n","their         2,037\n","digital       3,617\n","platforms     7,248\n",".             1,012\n","one           2,028\n","ai            9,932\n","technique     6,028\n","that          2,008\n","is            2,003\n","growing       3,652\n","in            1,999\n","its           2,049\n","application   4,646\n","and           1,998\n","use           2,224\n","is            2,003\n","chat         11,834\n","##bots       27,014\n",".             1,012\n","some          2,070\n","examples      4,973\n","of            1,997\n","chat         11,834\n","##bot        18,384\n","technology    2,974\n","are           2,024\n","virtual       7,484\n","assistants   16,838\n","like          2,066\n","amazon        9,733\n","'             1,005\n","s             1,055\n","alexa        24,969\n","and           1,998\n","google        8,224\n","assistant     3,353\n",",             1,010\n","and           1,998\n","messaging    24,732\n","apps         18,726\n",",             1,010\n","such          2,107\n","as            2,004\n","we            2,057\n","##cha         7,507\n","##t           2,102\n","and           1,998\n","facebook      9,130\n","'             1,005\n","s             1,055\n","messenger    11,981\n",".             1,012\n","a             1,037\n","chat         11,834\n","##bot        18,384\n","is            2,003\n","an            2,019\n","automated    12,978\n","program       2,565\n","that          2,008\n","interact     11,835\n","##s           2,015\n","with          2,007\n","customers     6,304\n","as            2,004\n","a             1,037\n","human         2,529\n","would         2,052\n","and           1,998\n","costs         5,366\n","little        2,210\n","to            2,000\n","nothing       2,498\n","to            2,000\n","engage        8,526\n","with          2,007\n",".             1,012\n","chat         11,834\n","##bots       27,014\n","attend        5,463\n","to            2,000\n","customers     6,304\n","at            2,012\n","all           2,035\n","times         2,335\n","of            1,997\n","the           1,996\n","day           2,154\n","and           1,998\n","week          2,733\n","and           1,998\n","are           2,024\n","not           2,025\n","limited       3,132\n","by            2,011\n","time          2,051\n","or            2,030\n","a             1,037\n","physical      3,558\n","location      3,295\n",".             1,012\n","this          2,023\n","makes         3,084\n","its           2,049\n","implementation  7,375\n","appealing    16,004\n","to            2,000\n","a             1,037\n","lot           2,843\n","of            1,997\n","businesses    5,661\n","that          2,008\n","may           2,089\n","not           2,025\n","have          2,031\n","the           1,996\n","manpower     22,039\n","or            2,030\n","financial     3,361\n","resources     4,219\n","to            2,000\n","keep          2,562\n","employees     5,126\n","working       2,551\n","around        2,105\n","the           1,996\n","clock         5,119\n",".             1,012\n","chat         11,834\n","##bots       27,014\n","are           2,024\n","convenient   14,057\n","for           2,005\n","providing     4,346\n","customer      8,013\n","service       2,326\n","and           1,998\n","support       2,490\n","24            2,484\n","hours         2,847\n","a             1,037\n","day           2,154\n",",             1,010\n","7             1,021\n","days          2,420\n","a             1,037\n","week          2,733\n",".             1,012\n","they          2,027\n","also          2,036\n","free          2,489\n","up            2,039\n","phone         3,042\n","lines         3,210\n","and           1,998\n","are           2,024\n","far           2,521\n","less          2,625\n","expensive     6,450\n","over          2,058\n","the           1,996\n","long          2,146\n","run           2,448\n","than          2,084\n","hiring       14,763\n","people        2,111\n","to            2,000\n","perform       4,685\n","support       2,490\n",".             1,012\n","using         2,478\n","ai            9,932\n","and           1,998\n","natural       3,019\n","language      2,653\n","processing    6,364\n",",             1,010\n","chat         11,834\n","##bots       27,014\n","are           2,024\n","becoming      3,352\n","better        2,488\n","at            2,012\n","understanding  4,824\n","what          2,054\n","customers     6,304\n","want          2,215\n","and           1,998\n","providing     4,346\n","the           1,996\n","help          2,393\n","they          2,027\n","need          2,342\n",".             1,012\n","companies     3,316\n","also          2,036\n","like          2,066\n","chat         11,834\n","##bots       27,014\n","because       2,138\n","they          2,027\n","can           2,064\n","collect       8,145\n","data          2,951\n","about         2,055\n","customer      8,013\n","que          10,861\n","##ries        5,134\n",",             1,010\n","response      3,433\n","times         2,335\n",",             1,010\n","satisfaction  9,967\n",",             1,010\n","and           1,998\n","so            2,061\n","on            2,006\n",".             1,012\n","chat         11,834\n","##bots       27,014\n",",             1,010\n","however       2,174\n",",             1,010\n","are           2,024\n","still         2,145\n","limited       3,132\n",".             1,012\n","even          2,130\n","with          2,007\n","natural       3,019\n","language      2,653\n","processing    6,364\n",",             1,010\n","they          2,027\n","may           2,089\n","not           2,025\n","fully         3,929\n","comprehend   22,346\n","a             1,037\n","customer      8,013\n","'             1,005\n","s             1,055\n","input         7,953\n","and           1,998\n","may           2,089\n","provide       3,073\n","inc           4,297\n","##oh         11,631\n","##ere         7,869\n","##nt          3,372\n","answers       6,998\n",".             1,012\n","many          2,116\n","chat         11,834\n","##bots       27,014\n","are           2,024\n","also          2,036\n","limited       3,132\n","in            1,999\n","the           1,996\n","scope         9,531\n","of            1,997\n","que          10,861\n","##ries        5,134\n","that          2,008\n","they          2,027\n","are           2,024\n","able          2,583\n","to            2,000\n","respond       6,869\n","to            2,000\n",".             1,012\n","this          2,023\n","may           2,089\n","lead          2,599\n","to            2,000\n","frustration   9,135\n","with          2,007\n","a             1,037\n","lack          3,768\n","of            1,997\n","emotion       7,603\n",",             1,010\n","sympathy     11,883\n",",             1,010\n","and           1,998\n","personal      3,167\n","##ization     3,989\n","given         2,445\n","fairly        7,199\n","generic      12,391\n","feedback     12,247\n",".             1,012\n","in            1,999\n","addition      2,804\n","to            2,000\n","customer      8,013\n","dissatisfaction 28,237\n","with          2,007\n","not           2,025\n","reaching      4,285\n","a             1,037\n","human         2,529\n","being         2,108\n",",             1,010\n","chat         11,834\n","##bots       27,014\n","can           2,064\n","be            2,022\n","expensive     6,450\n","to            2,000\n","implement    10,408\n","and           1,998\n","maintain      5,441\n",",             1,010\n","especially    2,926\n","if            2,065\n","they          2,027\n","must          2,442\n","be            2,022\n","customized   28,749\n","and           1,998\n","updated       7,172\n","often         2,411\n",".             1,012\n","\n","[SEP]           102\n","\n"]}]},{"cell_type":"markdown","source":["La question et pdf_txt ( le texte) peuvent être concatener ensemble, mais BERT a toujours besoin d'un moyen de les distinguer. BERT a deux encastrements spéciaux \"Segment\", un pour le segment \"A\" et un pour le segment \"B\". Avant que les intégrations de mots n'entrent dans les couches BERT, l'intégration du segment A doit être ajoutée aux jetons de question, et l'intégration du segment B doit être ajoutée à chacun des jetons answer_text.\n","Celles-ci sont gérées par la bibliothèque de transformateurs et tout ce que nous avons à faire est de spécifier '0' et '1' pour le jeton."],"metadata":{"id":"PKM1QqEqkMnb"}},{"cell_type":"code","source":["sep_index = input_ids.index(tokenizer.sep_token_id)\n","num_seg_a = sep_index + 1\n","num_seg_b = len(input_ids) - num_seg_a\n","#Here We Construct the list of 0s and 1s.\n","segment_ids = [0]*num_seg_a + [1]*num_seg_b\n","# There should be a segment_id for every input token.\n","assert len(segment_ids) == len(input_ids)"],"metadata":{"id":"rXRziKnT_R63"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ici, nous alimentons pdf_txt et la question dans le modèle"],"metadata":{"id":"jwNaIzvHkh48"}},{"cell_type":"code","source":["start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))"],"metadata":{"id":"c8G6BYtQ_R9O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ici, nous trouvons les scores 'début' et 'fin' les plus élevés et combinons les jetons dans la réponse et imprimons la réponse"],"metadata":{"id":"ZL7v2T5ekuNJ"}},{"cell_type":"code","source":["answer_start = torch.argmax(start_scores)\n","answer_end = torch.argmax(end_scores)\n","\n","answer = ' '.join(tokens[answer_start:answer_end+1])\n","print ('Question \"' + question + '\"' )\n","print('Answer: \"' + answer + '\"')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"orl2LjX-_R_v","executionInfo":{"status":"ok","timestamp":1668078605562,"user_tz":-60,"elapsed":188,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"867c8259-88bb-4080-b84a-f02a958b5390"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Question \"What are advantages of chatbot ?\"\n","Answer: \"chat ##bots are convenient for providing customer service and support 24 hours a day , 7 days a week . they also free up phone lines and are far less expensive over the long run than hiring people to perform support\"\n"]}]},{"cell_type":"markdown","source":["Nous pouvons reconstruire tous les mots qui ont été décomposés en sous-mots."],"metadata":{"id":"HMlmuXm-k39L"}},{"cell_type":"code","source":["answer = tokens[answer_start]\n","answer = tokens[answer_start]\n","\n","# Select the remaining answer tokens and join them with whitespace.\n","for i in range(answer_start + 1, answer_end + 1):\n","  # If it's a subword token, then recombine it with the previous token.\n","    if tokens[i][0:2] == '##':\n","        answer += tokens[i][2:]\n","        # Otherwise, add a space then the token.\n","    else:\n","        answer += ' ' + tokens[i]\n","print('Answer: \"' + answer + '\"')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dakizn0cBWiP","executionInfo":{"status":"ok","timestamp":1668078615272,"user_tz":-60,"elapsed":187,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"6296a62e-ba3c-4f97-9ee6-52dbb993625e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer: \"chatbots are convenient for providing customer service and support 24 hours a day , 7 days a week . they also free up phone lines and are far less expensive over the long run than hiring people to perform support\"\n"]}]},{"cell_type":"markdown","source":["Cette partie est une fonction regroupant les commandes précédentes"],"metadata":{"id":"flrSsB41lDDY"}},{"cell_type":"code","source":["def answer_question(question, pdf_txt):\n","\n","    input_ids = tokenizer.encode(question, pdf_txt, max_length=512, truncation=True)\n","\n","    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n","\n","    sep_index = input_ids.index(tokenizer.sep_token_id)\n","\n","    num_seg_a = sep_index + 1\n","\n","    num_seg_b = len(input_ids) - num_seg_a\n","\n","    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n","\n","    assert len(segment_ids) == len(input_ids)\n","\n","    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n","\n","    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","\n","    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n","    #print(f'score: {torch.max(start_scores)}')\n","    score = float(torch.max(start_scores))\n","    answer_start = torch.argmax(start_scores)\n","    answer_end = torch.argmax(end_scores)\n","    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","    answer = tokens[answer_start]\n","\n","    for i in range(answer_start + 1, answer_end + 1):\n","\n","        if tokens[i][0:2] == ' ':\n","            answer += tokens[i][2:]\n","\n","        else:\n","            answer += ' ' + tokens[i]\n","    return answer, score\n","    print('Answer: \"' + answer + '\"')"],"metadata":{"id":"qyQJ5SrL_SEJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["answer_question(question, pdf_txt)\n","# On applique la fonction sur la question et le texte pour obtenir une réponse"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quWSkKNX_byx","executionInfo":{"status":"ok","timestamp":1668078642026,"user_tz":-60,"elapsed":6431,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"827f42c0-ef7e-4742-a6ad-894b5d3a42af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query has 459 tokens.\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["('chat ##bots are convenient for providing customer service and support 24 hours a day , 7 days a week . they also free up phone lines and are far less expensive over the long run than hiring people to perform support',\n"," 4.012147903442383)"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["* Ici, nous utilisons la méthode tokenizers encode_plus pour créer nos jetons à partir de la chaîne de texte\n","* add_special_tokens=True ajoute des jetons BERT spéciaux comme [CLS], [SEP] et [PAD] à nos nouveaux encodages \"tokénisés\"\n","* max_length=512 indique à l'encodeur la longueur cible de nos encodages\n","* truncation=True garantit que nous coupons toutes les séquences qui sont plus longues que le\n","max_length spécifié.\n","* padding=\"max_length\" indique à l'encodeur de remplir toutes les séquences plus courtes que le max_length avec des jetons de remplissage."],"metadata":{"id":"bJGcCShXlfw0"}},{"cell_type":"code","source":["tokens = tokenizer.encode_plus(question, pdf_txt, add_special_tokens=True, max_length=512, truncation=True, padding=\"max_length\")\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HzYIVilC_b1h","executionInfo":{"status":"ok","timestamp":1668078650754,"user_tz":-60,"elapsed":213,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"fe67b643-6622-4674-a0ee-70a256bb54b6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 2054, 2024, 12637, 1997, 11834, 18384, 1029, 102, 1037, 11834, 18384, 2003, 1037, 3274, 2565, 2008, 26633, 2015, 2529, 4512, 2083, 2376, 10954, 2030, 3793, 11834, 2015, 2030, 2119, 1012, 11834, 18384, 1010, 2460, 2005, 24691, 18384, 1010, 2003, 2019, 7976, 4454, 1006, 9932, 1007, 3444, 2008, 2064, 2022, 11157, 1998, 2109, 2083, 2151, 2350, 24732, 4646, 1012, 2045, 2024, 1037, 2193, 1997, 10675, 2015, 2005, 11834, 18384, 1010, 2164, 1000, 2831, 18384, 1010, 1000, 1000, 28516, 1010, 1000, 1000, 10047, 28516, 1010, 1000, 1000, 9123, 4005, 1000, 2030, 1000, 7976, 4512, 9178, 1012, 1000, 1996, 6555, 5083, 1997, 2974, 2038, 2464, 2019, 3623, 1999, 5661, 3048, 2013, 3151, 2000, 3617, 7248, 2000, 9099, 18908, 2007, 10390, 1012, 15106, 2083, 2974, 2003, 2108, 3344, 2041, 2011, 5661, 2011, 14972, 9932, 5461, 2006, 2037, 3617, 7248, 1012, 2028, 9932, 6028, 2008, 2003, 3652, 1999, 2049, 4646, 1998, 2224, 2003, 11834, 27014, 1012, 2070, 4973, 1997, 11834, 18384, 2974, 2024, 7484, 16838, 2066, 9733, 1005, 1055, 24969, 1998, 8224, 3353, 1010, 1998, 24732, 18726, 1010, 2107, 2004, 2057, 7507, 2102, 1998, 9130, 1005, 1055, 11981, 1012, 1037, 11834, 18384, 2003, 2019, 12978, 2565, 2008, 11835, 2015, 2007, 6304, 2004, 1037, 2529, 2052, 1998, 5366, 2210, 2000, 2498, 2000, 8526, 2007, 1012, 11834, 27014, 5463, 2000, 6304, 2012, 2035, 2335, 1997, 1996, 2154, 1998, 2733, 1998, 2024, 2025, 3132, 2011, 2051, 2030, 1037, 3558, 3295, 1012, 2023, 3084, 2049, 7375, 16004, 2000, 1037, 2843, 1997, 5661, 2008, 2089, 2025, 2031, 1996, 22039, 2030, 3361, 4219, 2000, 2562, 5126, 2551, 2105, 1996, 5119, 1012, 11834, 27014, 2024, 14057, 2005, 4346, 8013, 2326, 1998, 2490, 2484, 2847, 1037, 2154, 1010, 1021, 2420, 1037, 2733, 1012, 2027, 2036, 2489, 2039, 3042, 3210, 1998, 2024, 2521, 2625, 6450, 2058, 1996, 2146, 2448, 2084, 14763, 2111, 2000, 4685, 2490, 1012, 2478, 9932, 1998, 3019, 2653, 6364, 1010, 11834, 27014, 2024, 3352, 2488, 2012, 4824, 2054, 6304, 2215, 1998, 4346, 1996, 2393, 2027, 2342, 1012, 3316, 2036, 2066, 11834, 27014, 2138, 2027, 2064, 8145, 2951, 2055, 8013, 10861, 5134, 1010, 3433, 2335, 1010, 9967, 1010, 1998, 2061, 2006, 1012, 11834, 27014, 1010, 2174, 1010, 2024, 2145, 3132, 1012, 2130, 2007, 3019, 2653, 6364, 1010, 2027, 2089, 2025, 3929, 22346, 1037, 8013, 1005, 1055, 7953, 1998, 2089, 3073, 4297, 11631, 7869, 3372, 6998, 1012, 2116, 11834, 27014, 2024, 2036, 3132, 1999, 1996, 9531, 1997, 10861, 5134, 2008, 2027, 2024, 2583, 2000, 6869, 2000, 1012, 2023, 2089, 2599, 2000, 9135, 2007, 1037, 3768, 1997, 7603, 1010, 11883, 1010, 1998, 3167, 3989, 2445, 7199, 12391, 12247, 1012, 1999, 2804, 2000, 8013, 28237, 2007, 2025, 4285, 1037, 2529, 2108, 1010, 11834, 27014, 2064, 2022, 6450, 2000, 10408, 1998, 5441, 1010, 2926, 2065, 2027, 2442, 2022, 28749, 1998, 7172, 2411, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"]},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","source":["Il renvoie un dictionnaire contenant trois paires clé-valeur, input_ids,\n","token_type_ids et attention_mask .\n","Nous avons également ajouté return_tensors='pt' pour renvoyer les tenseurs PyTorch du\n","tokenizer (plutôt que des listes Python)."],"metadata":{"id":"oX3FZNVrlx5j"}},{"cell_type":"code","source":["tokens = tokenizer.encode_plus(pdf_txt, add_special_tokens=False, return_tensors='pt')\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X3ae8IAkFVBF","executionInfo":{"status":"ok","timestamp":1668078654394,"user_tz":-60,"elapsed":201,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"f5991479-cb9d-4e62-ef3a-d2c37c5383fb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[ 1037, 11834, 18384,  2003,  1037,  3274,  2565,  2008, 26633,  2015,\n","          2529,  4512,  2083,  2376, 10954,  2030,  3793, 11834,  2015,  2030,\n","          2119,  1012, 11834, 18384,  1010,  2460,  2005, 24691, 18384,  1010,\n","          2003,  2019,  7976,  4454,  1006,  9932,  1007,  3444,  2008,  2064,\n","          2022, 11157,  1998,  2109,  2083,  2151,  2350, 24732,  4646,  1012,\n","          2045,  2024,  1037,  2193,  1997, 10675,  2015,  2005, 11834, 18384,\n","          1010,  2164,  1000,  2831, 18384,  1010,  1000,  1000, 28516,  1010,\n","          1000,  1000, 10047, 28516,  1010,  1000,  1000,  9123,  4005,  1000,\n","          2030,  1000,  7976,  4512,  9178,  1012,  1000,  1996,  6555,  5083,\n","          1997,  2974,  2038,  2464,  2019,  3623,  1999,  5661,  3048,  2013,\n","          3151,  2000,  3617,  7248,  2000,  9099, 18908,  2007, 10390,  1012,\n","         15106,  2083,  2974,  2003,  2108,  3344,  2041,  2011,  5661,  2011,\n","         14972,  9932,  5461,  2006,  2037,  3617,  7248,  1012,  2028,  9932,\n","          6028,  2008,  2003,  3652,  1999,  2049,  4646,  1998,  2224,  2003,\n","         11834, 27014,  1012,  2070,  4973,  1997, 11834, 18384,  2974,  2024,\n","          7484, 16838,  2066,  9733,  1005,  1055, 24969,  1998,  8224,  3353,\n","          1010,  1998, 24732, 18726,  1010,  2107,  2004,  2057,  7507,  2102,\n","          1998,  9130,  1005,  1055, 11981,  1012,  1037, 11834, 18384,  2003,\n","          2019, 12978,  2565,  2008, 11835,  2015,  2007,  6304,  2004,  1037,\n","          2529,  2052,  1998,  5366,  2210,  2000,  2498,  2000,  8526,  2007,\n","          1012, 11834, 27014,  5463,  2000,  6304,  2012,  2035,  2335,  1997,\n","          1996,  2154,  1998,  2733,  1998,  2024,  2025,  3132,  2011,  2051,\n","          2030,  1037,  3558,  3295,  1012,  2023,  3084,  2049,  7375, 16004,\n","          2000,  1037,  2843,  1997,  5661,  2008,  2089,  2025,  2031,  1996,\n","         22039,  2030,  3361,  4219,  2000,  2562,  5126,  2551,  2105,  1996,\n","          5119,  1012, 11834, 27014,  2024, 14057,  2005,  4346,  8013,  2326,\n","          1998,  2490,  2484,  2847,  1037,  2154,  1010,  1021,  2420,  1037,\n","          2733,  1012,  2027,  2036,  2489,  2039,  3042,  3210,  1998,  2024,\n","          2521,  2625,  6450,  2058,  1996,  2146,  2448,  2084, 14763,  2111,\n","          2000,  4685,  2490,  1012,  2478,  9932,  1998,  3019,  2653,  6364,\n","          1010, 11834, 27014,  2024,  3352,  2488,  2012,  4824,  2054,  6304,\n","          2215,  1998,  4346,  1996,  2393,  2027,  2342,  1012,  3316,  2036,\n","          2066, 11834, 27014,  2138,  2027,  2064,  8145,  2951,  2055,  8013,\n","         10861,  5134,  1010,  3433,  2335,  1010,  9967,  1010,  1998,  2061,\n","          2006,  1012, 11834, 27014,  1010,  2174,  1010,  2024,  2145,  3132,\n","          1012,  2130,  2007,  3019,  2653,  6364,  1010,  2027,  2089,  2025,\n","          3929, 22346,  1037,  8013,  1005,  1055,  7953,  1998,  2089,  3073,\n","          4297, 11631,  7869,  3372,  6998,  1012,  2116, 11834, 27014,  2024,\n","          2036,  3132,  1999,  1996,  9531,  1997, 10861,  5134,  2008,  2027,\n","          2024,  2583,  2000,  6869,  2000,  1012,  2023,  2089,  2599,  2000,\n","          9135,  2007,  1037,  3768,  1997,  7603,  1010, 11883,  1010,  1998,\n","          3167,  3989,  2445,  7199, 12391, 12247,  1012,  1999,  2804,  2000,\n","          8013, 28237,  2007,  2025,  4285,  1037,  2529,  2108,  1010, 11834,\n","         27014,  2064,  2022,  6450,  2000, 10408,  1998,  5441,  1010,  2926,\n","          2065,  2027,  2442,  2022, 28749,  1998,  7172,  2411,  1012]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["input_id_chunks = tokens['input_ids'][0].split(510)\n","mask_chunks = tokens['attention_mask'][0].split(510)\n","\n","for tensor in input_id_chunks:\n","  print(len(tensor))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qGjPLBWz_b4S","executionInfo":{"status":"ok","timestamp":1668078658770,"user_tz":-60,"elapsed":194,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"1d4021d5-e229-4d22-bdcc-54c90c167ade"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["449\n"]}]},{"cell_type":"code","source":["a = torch.arange(10)\n","a"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jcFJOS2V_b6p","executionInfo":{"status":"ok","timestamp":1668078662728,"user_tz":-60,"elapsed":189,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"b68e3448-24d7-45ca-aedc-35914b3bbf69"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["torch.cat(\n","    [torch.Tensor([101]), a, torch.Tensor([102])]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44yegsjJ_b9J","executionInfo":{"status":"ok","timestamp":1668078666482,"user_tz":-60,"elapsed":264,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"cc56ceb4-9dce-483a-c602-fdf918656377"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([101.,   0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9., 102.])"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["***Préparer les morceaux***\n","\n","Nous avons maintenant notre tenseur tokenisé ; nous devons le diviser en morceaux ne dépassant pas 510 jetons. Nous choisissons 510 plutôt que 512 pour laisser deux places libres pour ajouter nos jetons [CLS] et [SEP].\n","\n","***Diviser***\n","\n","\n","Nous appliquons la méthode de fractionnement à la fois à nos ID d'entrée et aux tenseurs de masque d'attention (nous\n","n'ont pas besoin des ID de type de jeton et peuvent les supprimer). Nous avons maintenant trois morceaux pour chaque ensemble de tenseurs. Notez que nous devrons ajouter un rembourrage au dernier morceau car il ne satisfera pas la taille de tenseur de 512 requise par BERT.\n","\n","\n","***CLS et SEP***\n","\n","\n","Ensuite, nous ajoutons les jetons de début de séquence [CLS] et de séparateur [SEP]. Pour cela, nous pouvons utiliser la fonction torch.cat, qui concatène une liste de tenseurs.\n","Nos jetons sont déjà au format d'identification de jeton, nous pouvons donc nous référer aux jetons spéciaux\n","tableau ci-dessus pour créer les versions d'ID de jeton de nos jetons [CLS] et [SEP].\n","Parce que nous faisons cela pour plusieurs tenseurs, nous plaçons la fonction torch.cat dans\n","une boucle for et effectuer la concaténation pour chacun de nos morceaux individuellement.\n","De plus, nos blocs de masque d'attention sont concaténés avec des 1 au lieu de 101\n","et 102. Nous faisons cela parce que le masque d'attention ne contient pas d'ID de jeton mais\n","à la place un ensemble de 1 et de 0.\n","Les zéros dans le masque d'attention représentent l'emplacement des jetons de remplissage (que nous allons\n","add next), et comme [CLS] et [SEP] ne sont pas des jetons de remplissage, ils sont représentés\n","avec 1s.\n","\n","***Rembourrage***\n","\n","\n","Nous devons ajouter un rembourrage à nos morceaux de tenseur pour nous assurer qu'ils satisfont à la longueur de tenseur de 512 requise par BERT. Nos deux premiers morceaux ne nécessitent aucun rembourrage car ils satisfont déjà à cette exigence de longueur, mais les derniers morceaux le font.\n","Pour vérifier si un morceau nécessite un remplissage, nous ajoutons une instruction if qui vérifie la longueur du tenseur. Si le tenseur est plus court que 512 jetons, nous ajoutons un rembourrage à l'aide de la fonction torch.cat. Nous devrions ajouter cette déclaration à la même boucle for où nous ajoutons nos jetons [CLS] et [SEP] - si vous avez besoin d'aide, j'ai inclus les scripts complets à la fin de l'article."],"metadata":{"id":"-NLeJsganP-Z"}},{"cell_type":"code","source":["chunksize = 512\n","\n","input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n","mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n","\n","for i in range(len(input_id_chunks)):\n","    input_id_chunks[i] = torch.cat([\n","        torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n","    ])\n","    mask_chunks[i] = torch.cat([\n","        torch.tensor([1]), mask_chunks[i], torch.tensor([1])\n","    ])\n","\n","    pad_len = chunksize - input_id_chunks[i].shape[0]\n","    if pad_len > 0:\n","        input_id_chunks[i] = torch.cat([\n","            input_id_chunks[i], torch.Tensor([0] * pad_len)\n","        ])\n","        mask_chunks[i] = torch.cat([\n","            mask_chunks[i], torch.Tensor([0] * pad_len)\n","        ])\n","\n","for chunk in input_id_chunks:\n","    print(chunk)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JSG7d6xs_cAJ","executionInfo":{"status":"ok","timestamp":1668078671204,"user_tz":-60,"elapsed":194,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"dd757780-bb9a-4417-86dd-878bb1d3d09d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([  101.,  1037., 11834., 18384.,  2003.,  1037.,  3274.,  2565.,  2008.,\n","        26633.,  2015.,  2529.,  4512.,  2083.,  2376., 10954.,  2030.,  3793.,\n","        11834.,  2015.,  2030.,  2119.,  1012., 11834., 18384.,  1010.,  2460.,\n","         2005., 24691., 18384.,  1010.,  2003.,  2019.,  7976.,  4454.,  1006.,\n","         9932.,  1007.,  3444.,  2008.,  2064.,  2022., 11157.,  1998.,  2109.,\n","         2083.,  2151.,  2350., 24732.,  4646.,  1012.,  2045.,  2024.,  1037.,\n","         2193.,  1997., 10675.,  2015.,  2005., 11834., 18384.,  1010.,  2164.,\n","         1000.,  2831., 18384.,  1010.,  1000.,  1000., 28516.,  1010.,  1000.,\n","         1000., 10047., 28516.,  1010.,  1000.,  1000.,  9123.,  4005.,  1000.,\n","         2030.,  1000.,  7976.,  4512.,  9178.,  1012.,  1000.,  1996.,  6555.,\n","         5083.,  1997.,  2974.,  2038.,  2464.,  2019.,  3623.,  1999.,  5661.,\n","         3048.,  2013.,  3151.,  2000.,  3617.,  7248.,  2000.,  9099., 18908.,\n","         2007., 10390.,  1012., 15106.,  2083.,  2974.,  2003.,  2108.,  3344.,\n","         2041.,  2011.,  5661.,  2011., 14972.,  9932.,  5461.,  2006.,  2037.,\n","         3617.,  7248.,  1012.,  2028.,  9932.,  6028.,  2008.,  2003.,  3652.,\n","         1999.,  2049.,  4646.,  1998.,  2224.,  2003., 11834., 27014.,  1012.,\n","         2070.,  4973.,  1997., 11834., 18384.,  2974.,  2024.,  7484., 16838.,\n","         2066.,  9733.,  1005.,  1055., 24969.,  1998.,  8224.,  3353.,  1010.,\n","         1998., 24732., 18726.,  1010.,  2107.,  2004.,  2057.,  7507.,  2102.,\n","         1998.,  9130.,  1005.,  1055., 11981.,  1012.,  1037., 11834., 18384.,\n","         2003.,  2019., 12978.,  2565.,  2008., 11835.,  2015.,  2007.,  6304.,\n","         2004.,  1037.,  2529.,  2052.,  1998.,  5366.,  2210.,  2000.,  2498.,\n","         2000.,  8526.,  2007.,  1012., 11834., 27014.,  5463.,  2000.,  6304.,\n","         2012.,  2035.,  2335.,  1997.,  1996.,  2154.,  1998.,  2733.,  1998.,\n","         2024.,  2025.,  3132.,  2011.,  2051.,  2030.,  1037.,  3558.,  3295.,\n","         1012.,  2023.,  3084.,  2049.,  7375., 16004.,  2000.,  1037.,  2843.,\n","         1997.,  5661.,  2008.,  2089.,  2025.,  2031.,  1996., 22039.,  2030.,\n","         3361.,  4219.,  2000.,  2562.,  5126.,  2551.,  2105.,  1996.,  5119.,\n","         1012., 11834., 27014.,  2024., 14057.,  2005.,  4346.,  8013.,  2326.,\n","         1998.,  2490.,  2484.,  2847.,  1037.,  2154.,  1010.,  1021.,  2420.,\n","         1037.,  2733.,  1012.,  2027.,  2036.,  2489.,  2039.,  3042.,  3210.,\n","         1998.,  2024.,  2521.,  2625.,  6450.,  2058.,  1996.,  2146.,  2448.,\n","         2084., 14763.,  2111.,  2000.,  4685.,  2490.,  1012.,  2478.,  9932.,\n","         1998.,  3019.,  2653.,  6364.,  1010., 11834., 27014.,  2024.,  3352.,\n","         2488.,  2012.,  4824.,  2054.,  6304.,  2215.,  1998.,  4346.,  1996.,\n","         2393.,  2027.,  2342.,  1012.,  3316.,  2036.,  2066., 11834., 27014.,\n","         2138.,  2027.,  2064.,  8145.,  2951.,  2055.,  8013., 10861.,  5134.,\n","         1010.,  3433.,  2335.,  1010.,  9967.,  1010.,  1998.,  2061.,  2006.,\n","         1012., 11834., 27014.,  1010.,  2174.,  1010.,  2024.,  2145.,  3132.,\n","         1012.,  2130.,  2007.,  3019.,  2653.,  6364.,  1010.,  2027.,  2089.,\n","         2025.,  3929., 22346.,  1037.,  8013.,  1005.,  1055.,  7953.,  1998.,\n","         2089.,  3073.,  4297., 11631.,  7869.,  3372.,  6998.,  1012.,  2116.,\n","        11834., 27014.,  2024.,  2036.,  3132.,  1999.,  1996.,  9531.,  1997.,\n","        10861.,  5134.,  2008.,  2027.,  2024.,  2583.,  2000.,  6869.,  2000.,\n","         1012.,  2023.,  2089.,  2599.,  2000.,  9135.,  2007.,  1037.,  3768.,\n","         1997.,  7603.,  1010., 11883.,  1010.,  1998.,  3167.,  3989.,  2445.,\n","         7199., 12391., 12247.,  1012.,  1999.,  2804.,  2000.,  8013., 28237.,\n","         2007.,  2025.,  4285.,  1037.,  2529.,  2108.,  1010., 11834., 27014.,\n","         2064.,  2022.,  6450.,  2000., 10408.,  1998.,  5441.,  1010.,  2926.,\n","         2065.,  2027.,  2442.,  2022., 28749.,  1998.,  7172.,  2411.,  1012.,\n","          102.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n","            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n","            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n","            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n","            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n","            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n","            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])\n"]}]},{"cell_type":"code","source":["for tensor in range(len(input_id_chunks)):\n","  ans = answer_question(question, ' '.join(tokenizer.convert_ids_to_tokens(input_id_chunks[tensor])))\n","  print(ans)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lx5Ydiwz_cG5","executionInfo":{"status":"ok","timestamp":1668078681971,"user_tz":-60,"elapsed":6601,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"76092bc7-413c-43d2-d4a0-c49582b7a985"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query has 512 tokens.\n","\n","('free up phone lines and are far less expensive over the long run than hiring people to perform support', 3.8189876079559326)\n"]}]},{"cell_type":"code","source":["def expand_split_sentences(pdf_txt):\n","  import nltk\n","  nltk.download('punkt')\n","  new_chunks = nltk.sent_tokenize(pdf_txt)\n","  length = len(new_chunks)\n","  #for i in range(length):\n","    #tmp_token = tokenizer.encode(new_chunks[i])\n","    #print('The input has a total of {:} tokens.'.format(len(tmp_token)))\n","\n","  new_df = [];\n","  for i in range(length):\n","    paragraph = \"\"\n","    for j in range(i, length):\n","      #tmp_str = paragraph + new_chunks[j]\n","      tmp_token = tokenizer.encode(paragraph + new_chunks[j])\n","      length_token = len(tmp_token)\n","      if length_token < 510:\n","        #print(length_token)\n","        paragraph = paragraph + new_chunks[j]\n","      else:\n","        #print(length_token)\n","        break;\n","    #print(len(tokenizer.encode(paragraph)))\n","    new_df.append(paragraph)\n","  return new_df\n","  #for i in new_df:\n","    #print(i)"],"metadata":{"id":"7ahE9X5AEydV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_score = 0;\n","final_answer = \"\"\n","new_df = expand_split_sentences(pdf_txt)\n","for new_context in new_df:\n","  #new_paragrapgh = new_paragrapgh + answer_question(question, answer_text)\n","  ans, score = answer_question(question, new_context)\n","  if score > max_score:\n","    max_score = score\n","    final_answer = ans\n","print(question)\n","print(final_answer)\n","print(max_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zf-Jx4ttEyfs","executionInfo":{"status":"ok","timestamp":1668078757039,"user_tz":-60,"elapsed":60016,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"af0e2175-0d22-48b5-d894-c9d530635887"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Query has 459 tokens.\n","\n","Query has 437 tokens.\n","\n","Query has 409 tokens.\n","\n","Query has 372 tokens.\n","\n","Query has 349 tokens.\n","\n","Query has 331 tokens.\n","\n","Query has 316 tokens.\n","\n","Query has 283 tokens.\n","\n","Query has 258 tokens.\n","\n","Query has 234 tokens.\n","\n","Query has 207 tokens.\n","\n","Query has 187 tokens.\n","\n","Query has 165 tokens.\n","\n","Query has 141 tokens.\n","\n","Query has 117 tokens.\n","\n","Query has 108 tokens.\n","\n","Query has 83 tokens.\n","\n","Query has 63 tokens.\n","\n","Query has 42 tokens.\n","\n","free up phone lines and are far less expensive over the long run than hiring people to perform support\n","5.080368995666504\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"e6DuanlvmxWK"}},{"cell_type":"markdown","source":["Nous pouvons voir que nous obtenons un ensemble de trois valeurs d'activation pour chaque morceau.\n","Ces valeurs d'activation ne sont pas encore nos probabilités de sortie. Pour les transformer en\n","probabilités de sortie, nous devons appliquer une fonction softmax au tenseur de sortie.\n","\n","Enfin, nous prenons la moyenne des valeurs de chaque classe (ou colonne) pour obtenir notre probabilité finale de sentiment positif, négatif ou neutre."],"metadata":{"id":"0V7UpVJ2muxM"}},{"cell_type":"code","source":["input_ids = torch.stack(input_id_chunks)\n","attention_mask = torch.stack(mask_chunks)\n","\n","input_dict = {\n","    'input_ids': input_ids.long(),\n","    'attention_mask': attention_mask.int()\n","}\n","input_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rowgM6TVEyic","executionInfo":{"status":"ok","timestamp":1668077605202,"user_tz":-60,"elapsed":221,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"f73c7be8-ba63-4290-8586-fb3e4330d11f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[  101,  2166,  5427,  ...,  1012,  2107,   102],\n","         [  101, 29361,  2024,  ...,  2064,  2022,   102],\n","         [  101,  3876,  2006,  ...,     0,     0,     0]]),\n"," 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","         [1, 1, 1,  ..., 1, 1, 1],\n","         [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["outputs = model(**input_dict)\n","probs = torch.nn.functional.softmax(outputs[0], dim=-1)\n","probs = probs.mean(dim=0)\n","probs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtlYsktoE8o2","executionInfo":{"status":"ok","timestamp":1668077629889,"user_tz":-60,"elapsed":21714,"user":{"displayName":"Achta Ngoma","userId":"06993674921622530539"}},"outputId":"934406b3-62c7-4cc8-a00f-5d657bda6bbf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([4.2047e-01, 7.3233e-03, 3.8704e-04, 3.7146e-04, 2.4710e-04, 4.9905e-04,\n","        9.1580e-05, 7.2339e-04, 2.8726e-04, 2.7150e-04, 2.2355e-04, 5.9827e-05,\n","        1.0277e-03, 5.1559e-05, 4.3810e-05, 1.1908e-04, 2.0978e-04, 1.3863e-04,\n","        4.2432e-05, 5.6722e-05, 7.6533e-05, 6.9248e-04, 9.3073e-05, 3.7591e-04,\n","        4.3682e-05, 4.0332e-04, 6.2978e-05, 8.4359e-05, 1.1684e-04, 4.9767e-05,\n","        2.4577e-04, 2.9513e-05, 1.8508e-04, 9.6008e-04, 1.3457e-04, 5.4936e-05,\n","        5.5473e-05, 1.6459e-04, 6.7666e-05, 1.0982e-04, 6.5894e-05, 5.0604e-05,\n","        8.9084e-05, 1.5366e-04, 4.3831e-05, 1.7032e-04, 6.9928e-04, 5.5394e-05,\n","        2.6971e-04, 4.2740e-05, 2.9958e-05, 8.3827e-05, 7.3587e-05, 1.0709e-04,\n","        5.4899e-04, 5.5681e-05, 3.9024e-05, 3.3720e-04, 4.9262e-05, 9.6260e-05,\n","        5.3931e-05, 1.5222e-04, 7.0258e-04, 2.5524e-03, 1.5948e-04, 1.0690e-03,\n","        6.7079e-05, 8.0789e-04, 2.0365e-03, 6.5701e-05, 1.2064e-04, 2.8996e-04,\n","        2.1347e-04, 5.6888e-05, 2.1249e-03, 3.9499e-05, 2.1159e-04, 2.6569e-04,\n","        6.2412e-05, 5.1129e-05, 8.9495e-05, 7.2415e-05, 3.5381e-03, 5.4556e-05,\n","        7.1508e-04, 1.4188e-04, 2.4499e-04, 5.9457e-05, 1.1119e-03, 2.0382e-04,\n","        1.4295e-04, 4.3345e-04, 1.3646e-04, 4.1674e-05, 2.0847e-04, 1.7065e-04,\n","        3.8519e-04, 3.8708e-04, 4.1200e-05, 3.0841e-04, 1.2647e-04, 1.3637e-04,\n","        4.6881e-05, 3.1991e-05, 3.2145e-05, 4.4246e-05, 8.7806e-05, 5.6480e-05,\n","        5.8580e-05, 5.6653e-05, 4.8347e-05, 2.4965e-05, 4.6378e-05, 2.6379e-05,\n","        4.8670e-05, 1.6737e-04, 6.4618e-05, 5.6495e-05, 8.1759e-05, 4.4466e-05,\n","        1.0158e-04, 3.5585e-05, 6.0410e-05, 6.3875e-05, 1.7039e-04, 5.0404e-05,\n","        5.7500e-05, 5.5004e-05, 4.6491e-05, 5.0295e-05, 4.5397e-05, 5.2832e-05,\n","        5.3389e-05, 6.3324e-05, 1.3064e-04, 2.5749e-04, 5.2643e-05, 4.6944e-05,\n","        3.7305e-05, 4.8249e-05, 8.0733e-05, 8.4464e-05, 1.0670e-04, 7.3485e-05,\n","        7.4626e-05, 5.2369e-05, 6.9581e-05, 2.5934e-05, 3.6436e-05, 1.8811e-04,\n","        8.0685e-05, 3.2122e-05, 5.3236e-05, 4.0746e-04, 9.3873e-05, 3.9662e-05,\n","        8.7556e-05, 1.4731e-04, 1.3155e-04, 2.4929e-05, 7.9417e-05, 3.0528e-05,\n","        1.0242e-04, 3.5068e-05, 3.4698e-05, 4.1113e-05, 4.6601e-05, 1.1962e-04,\n","        3.4961e-05, 3.7392e-05, 7.1280e-05, 2.1064e-05, 4.6823e-05, 2.8664e-05,\n","        4.4135e-05, 2.2795e-04, 6.4747e-05, 2.8058e-05, 5.6868e-05, 9.5724e-05,\n","        1.1329e-04, 3.4201e-05, 5.0846e-05, 3.1480e-05, 6.0794e-05, 5.2622e-05,\n","        2.9664e-04, 2.6548e-05, 3.1076e-05, 2.2069e-04, 1.8944e-04, 4.1666e-05,\n","        9.3682e-05, 1.0285e-04, 6.4692e-05, 3.3199e-05, 3.2460e-04, 8.5656e-05,\n","        7.1954e-05, 4.7625e-05, 1.1412e-04, 1.2776e-04, 1.0165e-04, 2.7259e-04,\n","        3.9819e-05, 3.7262e-05, 2.3008e-04, 8.0705e-05, 7.4008e-05, 1.0812e-04,\n","        8.5720e-05, 8.4158e-05, 3.3531e-05, 1.0134e-04, 3.8595e-05, 4.5253e-05,\n","        6.6540e-05, 8.9126e-05, 7.6436e-05, 7.7764e-05, 8.5636e-05, 3.5595e-05,\n","        3.2802e-05, 3.5160e-05, 5.9794e-05, 4.3163e-05, 5.5477e-05, 5.5824e-05,\n","        1.3474e-04, 3.5574e-05, 4.1005e-05, 2.7552e-05, 3.3310e-05, 3.7466e-05,\n","        3.1794e-05, 6.9628e-05, 3.5483e-05, 6.0901e-05, 1.4809e-04, 1.5280e-04,\n","        2.0674e-04, 8.9104e-05, 3.1164e-05, 4.7400e-05, 4.0626e-05, 7.1526e-05,\n","        3.5956e-05, 3.9222e-05, 3.7219e-05, 4.4639e-05, 1.0312e-04, 2.9464e-05,\n","        2.4477e-05, 3.3950e-05, 5.8510e-05, 7.1361e-05, 3.4700e-05, 4.5197e-05,\n","        4.2812e-05, 4.8044e-05, 3.1948e-05, 3.0816e-05, 6.2858e-05, 5.7715e-05,\n","        3.5892e-05, 3.9153e-05, 3.8666e-05, 5.6248e-05, 2.2652e-05, 7.0105e-05,\n","        4.9782e-05, 3.5454e-05, 4.0896e-05, 3.4124e-05, 4.9317e-05, 2.8924e-05,\n","        4.0054e-05, 3.9652e-05, 5.1658e-05, 9.8789e-02, 3.5356e-05, 1.5939e-05,\n","        1.7316e-05, 2.0357e-05, 2.4843e-05, 1.9501e-05, 3.3287e-05, 3.2738e-05,\n","        1.4073e-05, 1.5898e-05, 1.8144e-05, 1.4344e-05, 1.5390e-05, 2.0967e-05,\n","        1.8793e-05, 2.4851e-05, 1.5335e-05, 3.0455e-05, 2.7499e-05, 1.8701e-05,\n","        2.5784e-05, 2.9815e-05, 4.0560e-05, 2.9167e-05, 2.1711e-05, 1.9326e-05,\n","        2.2574e-05, 2.0133e-05, 2.3609e-05, 1.9194e-05, 1.6072e-05, 1.5034e-05,\n","        3.8934e-05, 1.6530e-05, 2.1887e-05, 1.7048e-05, 1.8730e-05, 2.6361e-05,\n","        3.6828e-05, 3.3074e-05, 1.9038e-05, 2.5489e-05, 2.4382e-05, 3.3837e-05,\n","        2.4585e-05, 7.0282e-05, 6.9552e-05, 5.9061e-05, 5.0989e-05, 5.3752e-05,\n","        1.5297e-04, 9.9908e-05, 1.9112e-04, 4.5872e-05, 3.1400e-05, 1.9073e-05,\n","        4.9482e-05, 1.0397e-04, 6.5561e-05, 1.1692e-04, 6.9386e-05, 2.2301e-05,\n","        3.3553e-05, 3.3179e-05, 2.2132e-05, 2.5484e-05, 2.5358e-05, 1.9016e-05,\n","        2.2342e-05, 3.0359e-05, 2.5104e-05, 2.0813e-05, 2.2827e-05, 1.7814e-05,\n","        2.5872e-05, 4.0808e-05, 2.3079e-05, 2.4501e-05, 2.0192e-05, 2.7207e-05,\n","        3.6556e-05, 6.1634e-05, 3.0985e-05, 1.4299e-04, 3.3231e-05, 2.9805e-05,\n","        4.4890e-05, 3.6918e-05, 2.5100e-05, 1.8482e-05, 2.7753e-05, 1.9064e-05,\n","        2.5518e-05, 3.3453e-05, 4.1150e-05, 1.8053e-05, 2.1683e-05, 1.7240e-05,\n","        2.1360e-05, 3.2736e-05, 2.0746e-05, 2.7276e-05, 1.8640e-05, 2.2516e-05,\n","        1.9912e-05, 2.7437e-05, 1.6754e-05, 3.9655e-05, 4.4313e-05, 1.6155e-05,\n","        3.4294e-05, 5.0868e-05, 3.5840e-05, 4.2855e-05, 1.3945e-04, 2.0258e-04,\n","        2.4367e-05, 3.2675e-05, 2.2929e-05, 3.4300e-05, 4.2000e-05, 3.4378e-05,\n","        2.3583e-05, 8.6818e-05, 2.5717e-05, 2.4171e-05, 2.9486e-05, 2.4192e-05,\n","        5.4532e-05, 2.9652e-05, 2.3284e-05, 5.6041e-05, 2.6010e-05, 2.5898e-05,\n","        2.5589e-05, 1.9638e-05, 2.5982e-05, 2.2606e-05, 6.2927e-05, 2.6647e-05,\n","        3.1055e-05, 2.5189e-05, 2.0966e-05, 7.5393e-05, 1.8333e-05, 1.9678e-05,\n","        2.8083e-05, 1.5749e-05, 2.1385e-05, 2.1441e-05, 2.4648e-05, 2.5035e-05,\n","        1.9541e-05, 4.0760e-05, 2.2986e-05, 3.7738e-05, 3.1105e-05, 2.7221e-05,\n","        3.3813e-05, 4.3472e-05, 2.7978e-05, 2.3420e-05, 3.4056e-05, 9.8847e-05,\n","        2.5885e-05, 4.9962e-05, 7.6817e-05, 2.4763e-05, 2.6551e-05, 3.4203e-05,\n","        3.8391e-05, 4.1616e-05, 6.4324e-05, 2.1028e-04, 3.3907e-05, 2.9787e-05,\n","        2.4334e-05, 3.2796e-05, 4.8975e-05, 3.9476e-05, 9.2538e-05, 2.9322e-05,\n","        3.0799e-05, 2.7479e-05, 2.6269e-05, 6.6660e-05, 3.5829e-05, 3.3094e-05,\n","        3.8299e-05, 2.2342e-05, 1.7251e-05, 2.6254e-05, 2.7984e-05, 1.2164e-04,\n","        1.9955e-05, 3.0520e-05, 2.3537e-05, 3.3842e-05, 2.5507e-05, 2.4733e-05,\n","        2.1638e-05, 2.5656e-05, 7.0366e-05, 1.9037e-05, 2.3059e-05, 1.8690e-05,\n","        3.8114e-05, 2.8532e-05, 2.7036e-05, 2.3507e-05, 2.6660e-05, 3.6071e-05,\n","        2.6525e-05, 2.1497e-05, 1.9108e-05, 2.7563e-05, 2.2473e-05, 2.9002e-05,\n","        2.2893e-05, 1.8821e-05, 2.0394e-05, 1.9089e-05, 2.3703e-05, 3.6839e-05,\n","        1.9838e-05, 1.7759e-05, 1.9908e-05, 3.5956e-05, 2.4703e-05, 2.0852e-05,\n","        5.0867e-04, 4.2030e-01], grad_fn=<MeanBackward1>)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":[],"metadata":{"id":"9XlD3zZUE8re"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"u81IKPIW_cJQ"},"execution_count":null,"outputs":[]}]}